Самым быстрым (и самым простым способом), по – видимому, является сопоставление чисел словам (т. н. ordinal encoding или label encoding), но в результате мы получаем не вектора и, к примеру, не можем использовать cosine similarity. 
Одним из методов также является One-hot encoding. Ключевой проблемой с one-hot encoding является то, что в итоге, при достаточно длинном тексте, мы получаем большой набор очень длинных векторов, тем самым засоряя память и увеличивая время на обработку. При этом, мы особо не приобретаем ничего – one hot encoding, в отличие от векторных моделей, не даст нам доп. информации о семантике/дистрибуции слова, с которой мы можем работать,  но будет работать быстрее, чем эмбеддинги . One-hot encoding модель, тем не менее, все равно будет сложнее и более времяёмкой, чем при ordinal encoding, так как мы пользуемся векторам. При one hot encoding Можно просто добавлять новые данные, но это приводит к curse of dimensionality – у нас получается по одному новому измерения на каждый новый вход, что усложняет обработку по сравнению с ordinal. 
При использовании эмбеддингов мы обычно получаем более короткие вектора (что несколько облегчит дальнейший процессинг и может сократить немного время на подсчёт),  но если мы не хотим просто достать готовый список векторов с RusVectores  и наложить его на наши данные, а хотим обучать модель на тексте (а так, как правило, бывает), то это требует большого времени. При добавлении новых данных модель придётся переобучать  - увеличивает  complexity и времяёмкость. В итоге такая модель будет, скорей всего, и самой времяёмкой, и самой complex (в т. ч. так как вектора связаны), но даст нам взамен ряд дополнительных возможностей. 
Самым быстрым и самым простым способом, является ordinal encoding он label encoding, потом бы (в общем случае) по этой шкале я бы поставил one-hot encoding и потом эмбеддинги (опять же среди эмбеддингов есть более быстрые и простые типа Word2Vec и более сложные и длинные типа Elmo и Berta).
