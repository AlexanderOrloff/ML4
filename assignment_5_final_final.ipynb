{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_5_final_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgrMz2NwLfQx",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfPsPIWZLdcP",
        "colab_type": "text"
      },
      "source": [
        "Build CNN model for sentiment analysis (binary classification) of IMDB Reviews (https://www.kaggle.com/utathya/imdb-review-dataset). You can use data with label=\"unsup\" for pretraining of embeddings. Here you are forbidden to use test dataset for pretraining of embeddings.\n",
        "Your quality metric is accuracy score on test dataset. Look at \"type\" column for train/test split.\n",
        "You can use pretrained embeddings from external sources.\n",
        "You have to provide data for trials with different hyperparameter values.\n",
        "\n",
        "You have to beat following baselines:\n",
        "[3 points] acc = 0.75\n",
        "[5 points] acc = 0.8\n",
        "[8 points] acc = 0.9\n",
        "\n",
        "[2 points] for using unsupervised data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhLoDq2qLXh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "en_stopwords = stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "import torch as tt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from sklearn import model_selection \n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-JEzfxnbkI8",
        "colab_type": "code",
        "outputId": "7649614b-95e2-4d5d-fba8-2738384f1114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tt.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDKCAnaXMZIc",
        "colab_type": "code",
        "outputId": "979e72df-3aa2-4d5e-cc56-7728b0961520",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb4cf6ec-dcc9-4a3e-a427-73c3a6d16860\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-eb4cf6ec-dcc9-4a3e-a427-73c3a6d16860\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving imdb-review-dataset.zip to imdb-review-dataset (1).zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HAwKvFVOCRP",
        "colab_type": "code",
        "outputId": "639ac2a0-8867-41f5-988e-58cbd000ebed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!unzip imdb-review-dataset.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  imdb-review-dataset.zip\n",
            "replace imdb_master.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: imdb_master.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nv50foMOClU",
        "colab_type": "code",
        "outputId": "612ed93b-b8c4-44e8-a976-a2df5c10d171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "data = pd.read_csv('imdb_master.csv', encoding='latin-1')\n",
        "data = data.drop('Unnamed: 0', axis = 1)\n",
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "      <td>0_2.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10000_4.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10001_1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10002_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test</td>\n",
              "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10003_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>train</td>\n",
              "      <td>Delightfully awful! Made by David Giancola, a ...</td>\n",
              "      <td>unsup</td>\n",
              "      <td>9998_0.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>train</td>\n",
              "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
              "      <td>unsup</td>\n",
              "      <td>9999_0.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>train</td>\n",
              "      <td>At the beginning we can see members of Troma t...</td>\n",
              "      <td>unsup</td>\n",
              "      <td>999_0.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>train</td>\n",
              "      <td>The movie was incredible, ever since I saw it ...</td>\n",
              "      <td>unsup</td>\n",
              "      <td>99_0.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>train</td>\n",
              "      <td>TCM came through by acquiring this wonderful, ...</td>\n",
              "      <td>unsup</td>\n",
              "      <td>9_0.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        type  ...         file\n",
              "0       test  ...      0_2.txt\n",
              "1       test  ...  10000_4.txt\n",
              "2       test  ...  10001_1.txt\n",
              "3       test  ...  10002_3.txt\n",
              "4       test  ...  10003_3.txt\n",
              "...      ...  ...          ...\n",
              "99995  train  ...   9998_0.txt\n",
              "99996  train  ...   9999_0.txt\n",
              "99997  train  ...    999_0.txt\n",
              "99998  train  ...     99_0.txt\n",
              "99999  train  ...      9_0.txt\n",
              "\n",
              "[100000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq9Se4Z_nCuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_data = data[(data['type'] == 'train') & (data['label'] != 'unsup')]\n",
        "withunsup_data  =   data[(data['type'] == 'train') ]\n",
        "test_data = data[(data['type'] == 'test')]\n",
        "train_data, val_data = model_selection.train_test_split(clean_data, test_size=0.05, stratify=clean_data.label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IouiBiGfOt1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object): \n",
        "    def __init__(self, input_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e3DydP1Pgo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, itos, unk_index):\n",
        "        self._itos = itos\n",
        "        self._stoi = {word:i for i, word in enumerate(itos)}\n",
        "        self._unk_index = unk_index\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._itos)\n",
        "    \n",
        "    def word2id(self, word):\n",
        "        idx = self._stoi.get(word)\n",
        "        if idx is not None:\n",
        "            return idx\n",
        "        return self._unk_index\n",
        "    \n",
        "    def id2word(self, idx):\n",
        "        return self._itos[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG9J-waVPtHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreProc():\n",
        "    def __init__(self, max_vocab_size):\n",
        "        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n",
        "        self.unk_index = 1\n",
        "        self.pad_index = 0\n",
        "        self.vocab = None\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        \n",
        "    def tokenize(self, text):\n",
        "        text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "        text = text.lower()\n",
        "        text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "        text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "        text = [word for word in text if word not in en_stopwords]\n",
        "        return text\n",
        "        \n",
        "    def build_vocab(self, tokens):\n",
        "        itos = []\n",
        "        itos.extend(self.special_words)\n",
        "        \n",
        "        token_counts = Counter(tokens)\n",
        "        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n",
        "            itos.append(word)\n",
        "            \n",
        "        self.vocab = Vocab(itos, self.unk_index)\n",
        "    \n",
        "    def transform(self, texts):\n",
        "        result = []\n",
        "        for text in texts:\n",
        "            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n",
        "            ids = [self.vocab.word2id(token) for token in tokens]\n",
        "            result.append(ids)\n",
        "        return result\n",
        "    \n",
        "    def fit_transform(self, texts):\n",
        "        result = []\n",
        "        tokenized_texts = [self.tokenize(text) for text in texts]\n",
        "        self.build_vocab(itertools.chain(*tokenized_texts))\n",
        "\n",
        "        for tokens in tokenized_texts:\n",
        "            tokens = ['<S>'] + tokens + ['</S>']\n",
        "            ids = [self.vocab.word2id(token) for token in tokens]\n",
        "            result.append(ids)\n",
        "\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyuGNKNQQEAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n",
        "    if len(token_ids) >= max_seq_len:\n",
        "        ids = token_ids[:max_seq_len]\n",
        "    else:\n",
        "        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n",
        "    return InputFeatures(ids, label_encoding[label])\n",
        "\n",
        "\n",
        "def features_to_tensor(features):\n",
        "    text_tensor = tt.tensor([example.input_ids for example in features], dtype=tt.long)\n",
        "    labels_tensor = tt.tensor([example.label_id for example in features], dtype=tt.long)\n",
        "    return text_tensor, labels_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okgmcShUQcui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_len=200\n",
        "classes = {'neg': 0, 'pos' : 1}\n",
        "text2id = PreProc(10000)\n",
        "\n",
        "train_ids = text2id.fit_transform(train_data['review'])\n",
        "val_ids = text2id.transform(val_data['review'])\n",
        "test_ids = text2id.transform(test_data['review'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgN1aA9RUY8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n",
        "                  for token_ids, label in zip(train_ids, train_data['label'])]\n",
        "\n",
        "val_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n",
        "                  for token_ids, label in zip(val_ids, val_data['label'])]\n",
        "\n",
        "test_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n",
        "                  for token_ids, label in zip(test_ids, test_data['label'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OULcgBUgUm-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTJppeF1U0UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tensor, train_labels = features_to_tensor(train_features)\n",
        "val_tensor, val_labels = features_to_tensor(val_features)\n",
        "test_tensor, test_labels = features_to_tensor(test_features)\n",
        "\n",
        "train_dataset = TensorDataset(train_tensor, train_labels)\n",
        "val_dataset = TensorDataset(val_tensor, val_labels)\n",
        "test_dataset = TensorDataset(test_tensor, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC5551ybVGLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.preproc = nn.Sequential(\n",
        "            nn.Embedding(10000,50)\n",
        "        )\n",
        "        self.hidden = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=50, out_channels=60, kernel_size=3), \n",
        "            nn.ReLU(), nn.MaxPool1d(3,stride=2),      \n",
        "            nn.Conv1d(in_channels=60, out_channels=100, kernel_size=3), \n",
        "            nn.ReLU(), nn.MaxPool1d(3, stride=2))\n",
        "        \n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(4700,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        batch = x.size(0)\n",
        "        x = self.preproc(x)\n",
        "        x = x.transpose(2,1)\n",
        "        \n",
        "        y = self.hidden(x).view(batch, -1)\n",
        "        return  self.output(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M3Pm4WEVgOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(net,criterion,train_loader,val_loader,optimizer, epochs):\n",
        "    best=0\n",
        "    net.cuda()\n",
        "\n",
        "    for i in range(epochs):\n",
        "        tr_loss = 0\n",
        "        val_loss = 0\n",
        "        val_accuracy =0\n",
        "\n",
        "        for xx,yy in train_loader:\n",
        "            xx, yy = xx.cuda(), yy.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y = net.forward(xx)\n",
        "            loss = criterion(y,yy.float().view(len(yy),-1))\n",
        "            tr_loss += loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        tr_loss /= len(train_loader)\n",
        "    \n",
        "        with tt.no_grad():\n",
        "            for xx,yy in val_loader:\n",
        "    \n",
        "                all_preds = []\n",
        "                xx, yy = xx.cuda(), yy.cuda()\n",
        "                y = net.forward(xx)\n",
        "                loss = criterion(y,yy.float().view(len(yy),-1))\n",
        "                val_loss += loss\n",
        "  \n",
        "                for index in y:\n",
        "                    if index>0.5:\n",
        "                        all_preds.append(1)\n",
        "                    else:\n",
        "                        all_preds.append(0)\n",
        "      \n",
        "                yy = yy.cpu().numpy()\n",
        "                val_accuracy += accuracy_score(all_preds,yy)\n",
        "\n",
        "            val_accuracy /= len(val_loader)\n",
        "            if val_accuracy>best:\n",
        "                best = val_accuracy\n",
        "                tt.save(net.state_dict(), \"../model.py\")\n",
        "\n",
        "        print((f\"epoch: {i}, train: {tr_loss.item()}, val: {val_accuracy.item()}\"))\n",
        "    net.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vfwyM3vWIhu",
        "colab_type": "code",
        "outputId": "6c1294a4-5d3b-41fe-debd-f70c3ab9c369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = MyModel()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "fit(model,criterion,train_loader,val_loader,optimizer,70)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, train: 0.5948524475097656, val: 0.7797334558823529\n",
            "epoch: 1, train: 0.38715213537216187, val: 0.8418658088235293\n",
            "epoch: 2, train: 0.29508525133132935, val: 0.8443933823529411\n",
            "epoch: 3, train: 0.23098213970661163, val: 0.8410845588235294\n",
            "epoch: 4, train: 0.17585469782352448, val: 0.8520220588235293\n",
            "epoch: 5, train: 0.15666772425174713, val: 0.8388327205882353\n",
            "epoch: 6, train: 0.1455235779285431, val: 0.8448988970588236\n",
            "epoch: 7, train: 0.11544328182935715, val: 0.8370863970588236\n",
            "epoch: 8, train: 0.07831399887800217, val: 0.8550551470588236\n",
            "epoch: 9, train: 0.05606941506266594, val: 0.8573988970588236\n",
            "epoch: 10, train: 0.051253173500299454, val: 0.8612132352941175\n",
            "epoch: 11, train: 0.04713822528719902, val: 0.8613051470588236\n",
            "epoch: 12, train: 0.04420037567615509, val: 0.8626838235294118\n",
            "epoch: 13, train: 0.016389163210988045, val: 0.8573069852941175\n",
            "epoch: 14, train: 0.005285417661070824, val: 0.8566176470588236\n",
            "epoch: 15, train: 0.002388832625001669, val: 0.8627757352941176\n",
            "epoch: 16, train: 0.001068564597517252, val: 0.8604319852941176\n",
            "epoch: 17, train: 0.000655960466247052, val: 0.8604319852941176\n",
            "epoch: 18, train: 0.0004701007856056094, val: 0.8604319852941176\n",
            "epoch: 19, train: 0.00035225090687163174, val: 0.8604319852941176\n",
            "epoch: 20, train: 0.0002701445482671261, val: 0.8604319852941176\n",
            "epoch: 21, train: 0.00021009848569519818, val: 0.8596507352941176\n",
            "epoch: 22, train: 0.00016476736345794052, val: 0.8596507352941176\n",
            "epoch: 23, train: 0.00013010298425797373, val: 0.8596507352941176\n",
            "epoch: 24, train: 0.00010296736581949517, val: 0.8612132352941175\n",
            "epoch: 25, train: 8.187736966647208e-05, val: 0.8604319852941176\n",
            "epoch: 26, train: 6.523444608319551e-05, val: 0.8588694852941176\n",
            "epoch: 27, train: 5.2062474424019456e-05, val: 0.8588694852941176\n",
            "epoch: 28, train: 4.1550178139004856e-05, val: 0.8596507352941176\n",
            "epoch: 29, train: 3.3213276765309274e-05, val: 0.8612132352941175\n",
            "epoch: 30, train: 2.6562503990135156e-05, val: 0.8612132352941175\n",
            "epoch: 31, train: 2.128834603354335e-05, val: 0.8612132352941175\n",
            "epoch: 32, train: 1.706040529825259e-05, val: 0.8612132352941175\n",
            "epoch: 33, train: 1.367188451695256e-05, val: 0.8612132352941175\n",
            "epoch: 34, train: 1.0971255505864974e-05, val: 0.8612132352941175\n",
            "epoch: 35, train: 8.793798770057037e-06, val: 0.8619944852941176\n",
            "epoch: 36, train: 7.050624390103621e-06, val: 0.8619944852941176\n",
            "epoch: 37, train: 5.654603683069581e-06, val: 0.8619944852941176\n",
            "epoch: 38, train: 4.532477305474458e-06, val: 0.8619944852941176\n",
            "epoch: 39, train: 3.6354167605168186e-06, val: 0.8612132352941175\n",
            "epoch: 40, train: 2.910171588155208e-06, val: 0.8619944852941176\n",
            "epoch: 41, train: 2.3311283712246222e-06, val: 0.8619944852941176\n",
            "epoch: 42, train: 1.867502874119964e-06, val: 0.8619944852941176\n",
            "epoch: 43, train: 1.4940587789169513e-06, val: 0.8627757352941176\n",
            "epoch: 44, train: 1.1959095900238026e-06, val: 0.8627757352941176\n",
            "epoch: 45, train: 9.569965868649888e-07, val: 0.8627757352941176\n",
            "epoch: 46, train: 7.660558480893087e-07, val: 0.8619944852941176\n",
            "epoch: 47, train: 6.128044560682611e-07, val: 0.8619944852941176\n",
            "epoch: 48, train: 4.903398576061591e-07, val: 0.8619944852941176\n",
            "epoch: 49, train: 3.9182953059935244e-07, val: 0.8627757352941176\n",
            "epoch: 50, train: 3.1317256343754707e-07, val: 0.8619944852941176\n",
            "epoch: 51, train: 2.4958464450719475e-07, val: 0.8612132352941175\n",
            "epoch: 52, train: 1.9912792481591168e-07, val: 0.8612132352941175\n",
            "epoch: 53, train: 1.58510175651827e-07, val: 0.8612132352941175\n",
            "epoch: 54, train: 1.264996711825006e-07, val: 0.8619944852941176\n",
            "epoch: 55, train: 1.0092584545873251e-07, val: 0.8619944852941176\n",
            "epoch: 56, train: 8.094746561937427e-08, val: 0.8619944852941176\n",
            "epoch: 57, train: 6.482706282895379e-08, val: 0.8635569852941176\n",
            "epoch: 58, train: 5.203389719099505e-08, val: 0.8635569852941176\n",
            "epoch: 59, train: 4.1656662119748944e-08, val: 0.8635569852941176\n",
            "epoch: 60, train: 3.3409946809115354e-08, val: 0.8627757352941176\n",
            "epoch: 61, train: 2.6938257846609304e-08, val: 0.8627757352941176\n",
            "epoch: 62, train: 2.1643238312663016e-08, val: 0.8627757352941176\n",
            "epoch: 63, train: 1.746730582397049e-08, val: 0.8627757352941176\n",
            "epoch: 64, train: 1.3979853719092716e-08, val: 0.8627757352941176\n",
            "epoch: 65, train: 1.1323580295652391e-08, val: 0.8619944852941176\n",
            "epoch: 66, train: 9.122955724194526e-09, val: 0.8604319852941176\n",
            "epoch: 67, train: 7.337921381633805e-09, val: 0.8604319852941176\n",
            "epoch: 68, train: 5.930922419850049e-09, val: 0.8612132352941175\n",
            "epoch: 69, train: 4.6941663889299434e-09, val: 0.8619944852941176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIbQbLdrg5UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(test_loader):\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  with tt.no_grad():\n",
        "    model.eval()\n",
        "    for texts, labels in test_loader:\n",
        "        model.cuda()\n",
        "        texts = texts.cuda()\n",
        "        output = model.forward(texts)\n",
        "        for i in output:\n",
        "            if i>0.5:\n",
        "                y_pred.append(1)\n",
        "            else:\n",
        "                y_pred.append(0)\n",
        "        y_true.extend(labels.tolist())\n",
        "  return accuracy_score(y_pred, y_true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCydHxDrhSXB",
        "colab_type": "code",
        "outputId": "98cc8e8f-2ce9-4603-c8db-22bbd43cd247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "eval(test_loader)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}